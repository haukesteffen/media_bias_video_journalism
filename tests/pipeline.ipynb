{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtubesearchpython import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import glob\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTranscript(video_id):\n",
    "    '''\n",
    "    Downloads Transcript from YouTube video and returns it in a DataFrame.\n",
    "    :param video_id: String of YouTube video ID\n",
    "    :return: Pandas DataFrame of Transcript\n",
    "    '''\n",
    "    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['de', 'en'])\n",
    "    transcript_df = pd.DataFrame(transcript)\n",
    "    return transcript_df\n",
    "\n",
    "### initializing DataFrame of youtube channels to scrape captions data from\n",
    "channels_dict = {'name':['NachDenkSeiten', 'Spiegel', 'ZDFheute', 'BILD', 'Junge Freiheit'],\n",
    "                'id':['UCE7b8qctaEGmST38-sfdOsA', 'UC1w6pNGiiLdZgyNpXUnA4Zw', 'UCeqKIgPQfNInOswGRWt48kQ', 'UC4zcMHyrT_xyWlgy5WGpFFQ', 'UCXJBRgiZRZvfilIGQ4wN5CQ']}\n",
    "channels_df = pd.DataFrame(channels_dict)\n",
    "\n",
    "### iterating over channels in DataFrame\n",
    "for index, channel in channels_df.iterrows():\n",
    "    print('getting info on youtube channel ' + channel['name'] + '...')\n",
    "    playlist = Playlist(playlist_from_channel_id(channel['id']))\n",
    "\n",
    "    ### retrieving video ids from channel id\n",
    "    while playlist.hasMoreVideos:\n",
    "        try:\n",
    "            playlist.getNextVideos()\n",
    "        except:\n",
    "            pass\n",
    "    print(f'videos retrieved: {len(playlist.videos)}')\n",
    "\n",
    "    ### getting transcripts from video id\n",
    "    transcript_dict = {'id':[], 'transcript':[]}\n",
    "    for video in range(len(playlist.videos)):\n",
    "        text = ''\n",
    "        print('getting transcript of video number ' + str(video) + ' with id ' + playlist.videos[video]['id'])\n",
    "        try:\n",
    "            captions = getTranscript(playlist.videos[video]['id'])\n",
    "            captions = captions[captions['start'] >= 5.0] #start getting captions after 5s\n",
    "            for line in captions['text'] :\n",
    "                text += line + ' '\n",
    "            transcript_dict['id'].append(playlist.videos[video]['id'])\n",
    "            transcript_dict['transcript'].append(text)\n",
    "        except:\n",
    "            print('could not get transcript for video number ' + str(video) + ' with id ' + playlist.videos[video]['id'])\n",
    "\n",
    "    ### converting to dataframe and saving as csv\n",
    "    transcript_df = pd.DataFrame(transcript_dict)\n",
    "    print(transcript_df.head())\n",
    "    transcript_df.to_csv('data\\\\raw\\\\'+ channel['name'] + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    '''\n",
    "    tokenizes and lemmatizes german input text\n",
    "    :param text: raw input text (german)\n",
    "    :return: list of lemmatized tokens from input text\n",
    "    '''\n",
    "    doc = nlp(str(text))\n",
    "    lemmas_tmp = [token.lemma_.lower() for token in doc]\n",
    "    lemmas = [lemma for lemma in lemmas_tmp if lemma.isalpha() and lemma not in filterwords]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "### initializing spacy with german language\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "filterwords = spacy.lang.de.stop_words.STOP_WORDS\n",
    "filterwords.update(['musik', 'music', 'applause', 'applaus', 'tv',\n",
    "                    'bild',\n",
    "                    'spiegel',\n",
    "                    'nachdenkseiten',\n",
    "                    'junge freiheit', 'j ftv' , 'jfv', 'fjt v', 'jftv',\n",
    "                    'zdf', 'claus kleber'])\n",
    "\n",
    "### looping through input files\n",
    "path = 'data/raw/*.csv'\n",
    "for csv in glob.glob(path):\n",
    "    ### importing data\n",
    "    df = pd.read_csv(csv, index_col=0)\n",
    "\n",
    "    ### preprocess transcript data\n",
    "    df['preprocessed'] = df['transcript'].apply(preprocess)\n",
    "    df.to_csv(csv.replace('data\\\\','data\\\\preprocessed\\\\').replace('.csv','_preprocessed.csv'))\n",
    "\n",
    "\n",
    "### load data\n",
    "n_samples = [10, 50, 100, 300]\n",
    "rng_seed = 42\n",
    "path = 'data\\\\preprocessed/*.csv'\n",
    "for i, k in enumerate(n_samples):\n",
    "    data = pd.DataFrame()\n",
    "    for csv in glob.glob(path):\n",
    "        tmp = pd.read_csv(csv, index_col=0)\n",
    "        tmp = tmp.sample(n=k, random_state=rng_seed)\n",
    "        tmp['medium'] = csv.replace('data\\\\preprocessed\\\\', '').replace('_preprocessed.csv', '')\n",
    "        data = pd.concat([data, tmp])\n",
    "    data = shuffle(data, random_state=rng_seed).astype(str)\n",
    "    data.to_csv('data\\\\samples\\\\sample'+str(k)+'.csv')\n",
    "    del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VECTORIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_seed = 42\n",
    "\n",
    "### importing data\n",
    "df = pd.read_csv('data/samples/sample10.csv')\n",
    "data = df.groupby(['medium'])['preprocessed'].sum()\n",
    "data = data.loc[['NachDenkSeiten', 'Spiegel', 'ZDFheute', 'BILD', 'Junge Freiheit']]\n",
    "X = data.values\n",
    "y = data.index\n",
    "\n",
    "\n",
    "### instantiating vectorizers\n",
    "cv = CountVectorizer(ngram_range=(1,3))\n",
    "X_cv = cv.fit_transform(X)\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3))\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "X_we = [nlp(x).vector for x in X]\n",
    "\n",
    "hv = HashingVectorizer(ngram_range=(1,3))\n",
    "X_hv = hv.fit_transform(X)\n",
    "\n",
    "\n",
    "### pca for dimension reduction\n",
    "pca_cv = TruncatedSVD(n_components=2)\n",
    "features_cv = pca_cv.fit_transform(X_cv)\n",
    "xs_cv = features_cv[:,0]\n",
    "ys_cv = features_cv[:,1]\n",
    "\n",
    "pca_tfidf = TruncatedSVD(n_components=2)\n",
    "features_cv = pca_tfidf.fit_transform(X_tfidf)\n",
    "xs_tfidf = features_cv[:,0]\n",
    "ys_tfidf = features_cv[:,1]\n",
    "\n",
    "pca_we = TruncatedSVD(n_components=2)\n",
    "features_we = pca_we.fit_transform(X_we)\n",
    "xs_we = features_we[:,0]\n",
    "ys_we = features_we[:,1]\n",
    "\n",
    "pca_hv = TruncatedSVD(n_components=2)\n",
    "features_hv = pca_hv.fit_transform((X_hv))\n",
    "xs_hv = features_hv[:,0]\n",
    "ys_hv = features_hv[:,1]\n",
    "\n",
    "\n",
    "### plots\n",
    "sns.set()\n",
    "sns.set_style('darkgrid')\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2)\n",
    "sns.scatterplot(x=xs_cv, y=ys_cv, hue=y, ax=axs[0, 0], palette='RdBu').set(title='Count Vectorizer')\n",
    "sns.scatterplot(x=xs_tfidf, y=ys_tfidf, hue=y, ax=axs[0, 1], palette='RdBu').set(title='TFIDF Vectorizer')\n",
    "sns.scatterplot(x=xs_we, y=ys_we, hue=y, ax=axs[1, 0], palette='RdBu').set(title='Spacy Word Embeddings Vectorizer')\n",
    "sns.scatterplot(x=xs_hv, y=ys_hv, hue=y, ax=axs[1, 1], palette='RdBu').set(title='Hashing Vectorizer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-Idf-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['zitat', 'medien', 'usa', 'ander', 'geben', 'us', 'russland',\n",
      "       'deutsch', 'politik', 'deutschland', 'prozent', 'menschen',\n",
      "       'artikel', 'zahlreiche', 'sagen', 'nato', 'leser', 'all',\n",
      "       'positiv', 'fragen', 'unterstützen', 'sanktionen', 'website',\n",
      "       'unterstützung', 'entstehen', 'fördern', 'spenden', 'hören',\n",
      "       'leserinnen', 'youtube', 'bewertung', 'assange', 'politisch',\n",
      "       'sehen', 'unternehmen', 'entstehen zahlreiche',\n",
      "       'entsprechend button', 'weiterverbreitung', 'button', 'soundcloud',\n",
      "       'audio', 'positiv bewertung', 'audio entstehen',\n",
      "       'klicken einfach entsprechend', 'itunes soundcloud',\n",
      "       'button website itunes', 'einfach entsprechend button',\n",
      "       'website itunes', 'website itunes soundcloud', 'abgeben freuen',\n",
      "       'button website', 'entsprechend button website',\n",
      "       'bewertung abgeben freuen', 'positiv bewertung abgeben',\n",
      "       'unterstützen klicken', 'unterstützen klicken einfach',\n",
      "       'audio entstehen zahlreiche', 'klicken einfach',\n",
      "       'bewertung abgeben', 'itunes', 'einfach entsprechend',\n",
      "       'fördern spenden unterstützen', 'spenden unterstützen klicken',\n",
      "       'spenden unterstützen', 'fördern spenden', 'einfach', 'klicken',\n",
      "       'leser leserinnen', 'abgeben freuen unterstützung',\n",
      "       'freuen unterstützung weiterverbreitung', 'abonnieren positiv',\n",
      "       'unterstützung weiterverbreitung', 'zahlreiche leser',\n",
      "       'abonnieren positiv bewertung', 'soundcloud youtube',\n",
      "       'soundcloud youtube hören', 'itunes soundcloud youtube',\n",
      "       'hören gerne kanal', 'youtube hören', 'youtube hören gerne',\n",
      "       'entstehen zahlreiche leser', 'kanal abonnieren positiv',\n",
      "       'freuen unterstützung', 'gerne kanal abonnieren', 'gerne kanal',\n",
      "       'leserinnen fördern spenden', 'leserinnen fördern',\n",
      "       'leser leserinnen fördern', 'zahlreiche leser leserinnen',\n",
      "       'entsprechend', 'hören gerne', 'euro', 'spd', 'kanal abonnieren',\n",
      "       'kanal', 'abgeben', 'gerne', 'stehen', 'mal', 'krieg'],\n",
      "      dtype=object)]\n",
      "[array(['the', 'sagen', 'geben', 'mal', 'and', 'to', 'sehen', 'all',\n",
      "       'ander', 'that', 'einfach', 'glauben', 'of', 'jed', 'mr speaker',\n",
      "       'speaker', 'leben', 'fragen', 'eigentlich', 'stehen', 'menschen',\n",
      "       'uh', 'we', 'finden', 'mr', 'leute', 'bisschen', 'kinder', 'is',\n",
      "       'deutschland', 'weg', 'halt', 'irgendwie', 'lassen', 'have',\n",
      "       'deutsch', 'heißen', 'frau', 'alt', 'stasi', 'klaren', 'this',\n",
      "       'polizei', 'bekommen', 'weiß', 'auto', 'patienten', 'bleiben',\n",
      "       'on', 'wissen', 'nehmen', 'euro', 'bringen', 'wichtig', 'genau',\n",
      "       'schwer', 'haus', 'sozusagen', 'problem', 'arbeiten', 'for',\n",
      "       'familie', 'neu', 'wasser', 'you', 'brauchen', 'letzt', 'it',\n",
      "       'versuchen', 'nächst', 'sprechen', 'kennen', 'denken', 'geld',\n",
      "       'fall', 'spielen', 'klein', 'laufen', 'land', 'fahren',\n",
      "       'passieren', 'angst', 'schön', 'zeigen', 'stunden', 'platz',\n",
      "       'wohnung', 'csu', 'tumor', 'flüchtlinge', 'with', 'paar',\n",
      "       'stimmenfang', 'hören', 'sitzen', 'liegen', 'moment', 'people',\n",
      "       'straße', 'mann'], dtype=object)]\n",
      "[array(['sagen', 'geben', 'mal', 'fragen', 'menschen', 'ander', 'all',\n",
      "       'sehen', 'impfen', 'glauben', 'deutschland', 'corona', 'virus',\n",
      "       'jed', 'impfstoff', 'einfach', 'stehen', 'abend', 'finden',\n",
      "       'ukraine', 'genau', 'eigentlich', 'prozent', 'wichtig', 'heißen',\n",
      "       'pandemie', 'deutsch', 'bleiben', 'grad', 'maßnahmen', 'land',\n",
      "       'klaren', 'tatsächlich', 'sprechen', 'leben', 'brauchen', 'neu',\n",
      "       'wissen', 'zahlen', 'impfung', 'bisschen', 'letzt', 'situation',\n",
      "       'russland', 'wochen', 'lassen', 'leute', 'nächst', 'deutlich',\n",
      "       'zeigen', 'liegen', 'kinder', 'nehmen', 'bringen', 'berlin',\n",
      "       'bekommen', 'stark', 'regierung', 'stellen', 'schnellen', 'halten',\n",
      "       'hören', 'herr', 'testen', 'lage', 'weg', 'gelten', 'treffen',\n",
      "       'weiß', 'entscheiden', 'welch', 'test', 'passieren', 'korona',\n",
      "       'thema', 'china', 'cdu', 'moment', 'schön', 'länder', 'gestern',\n",
      "       'laschet', 'woche', 'gemeinsam', 'denken', 'schwer', 'danken',\n",
      "       'union', 'fall', 'impfpflicht', 'europa', 'beid', 'infizieren',\n",
      "       'präsident', 'gleichen', 'problem', 'sozusagen', 'welt', 'einig',\n",
      "       'millionen'], dtype=object)]\n",
      "[array(['the', 'to', 'sagen', 'and', 'geben', 'mal', 'that', 'on', 'you',\n",
      "       'dragon', 'we', 'of', 'israel', 'sehen', 'laschet', 'applause',\n",
      "       'glauben', 'menschen', 'armin laschet', 'armin', 'space', 'aviv',\n",
      "       'is', 'all', 'for', 'space station', 'station', 'ander', 'hatch',\n",
      "       'raketen', 'international space', 'international space station',\n",
      "       'just', 'fragen', 'the international', 'sprechen', 'eigentlich',\n",
      "       'applause applause', 'einfach', 'the international space', 'jed',\n",
      "       'hamas', 'it', 'bisschen', 'now', 'genau', 'deutschland', 'from',\n",
      "       'this', 'get', 'have', 'stehen', 'passieren', 'leute', 'on the',\n",
      "       'letzt', 'of the', 'nächst', 'moment', 'finden', 'hören', 'klaren',\n",
      "       'halt', 'weiß', 'tatsächlich', 'beid', 'irgendwie', 'houston',\n",
      "       'to the', 'some', 'söder', 'one', 'be', 'wissen', 'here',\n",
      "       'situation', 'bekommen', 'markus söder', 'heißen', 'trump',\n",
      "       'denken', 'with', 'corona', 'sozusagen', 'joe biden', 'fall',\n",
      "       'biden', 'leben', 'joe', 'abend', 'stunden', 'international',\n",
      "       'donald', 'applause applause applause', 'up', 'through',\n",
      "       'donald trump', 'polizei', 'markus', 'are'], dtype=object)]\n",
      "[array(['sagen', 'geben', 'mal', 'ander', 'sehen', 'deutschland', 'afg',\n",
      "       'eigentlich', 'fragen', 'partei', 'freiheit', 'deutsch', 'all',\n",
      "       'glauben', 'stehen', 'thema', 'gender', 'buch', 'menschen', 'frau',\n",
      "       'heißen', 'leute', 'migranten', 'jed', 'finden', 'einfach',\n",
      "       'medien', 'jung', 'politik', 'nehmen', 'junge', 'politisch',\n",
      "       'genau', 'merkel', 'link', 'cdu', 'grün', 'sprechen', 'stellen',\n",
      "       'klaren', 'denken', 'land', 'europa', 'junge freiheit',\n",
      "       'tatsächlich', 'schreiben', 'parteien', 'geschichte', 'fdp',\n",
      "       'konservativ', 'afd', 'jung freiheit', 'seite', 'euro', 'zeigen',\n",
      "       'prozent', 'letzt', 'scholl', 'leben', 'neu', 'stein', 'fall',\n",
      "       'nämlich', 'sozusagen', 'einig', 'sogar', 'berlin', 'stark',\n",
      "       'erleben', 'weiß', 'danken', 'weg', 'hören', 'völlig', 'wichtig',\n",
      "       'deutlich', 'problem', 'halten', 'führen', 'bisschen', 'praktisch',\n",
      "       'sicherlich', 'krise', 'meinung', 'welch', 'dieter', 'lassen',\n",
      "       'aktuell', 'grunde', 'beid', 'frauen', 'mann', 'bundestag',\n",
      "       'fahren', 'dieter stein', 'europäisch', 'liegen', 'nächst',\n",
      "       'folgen', 'politische'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "def get_top_tf_idf_words(response, top_n=n):\n",
    "    sorted_nzs = np.argsort(response.data)[:-(top_n+1):-1]\n",
    "    return feature_names[response.indices[sorted_nzs]]\n",
    "\n",
    "### importing data\n",
    "df = pd.read_csv('data/samples/sample300.csv')\n",
    "data = df.groupby(['medium'])['preprocessed'].sum()\n",
    "data = data.loc[['NachDenkSeiten', 'Spiegel', 'ZDFheute', 'BILD', 'Junge Freiheit']]\n",
    "features = data.values\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3))\n",
    "X = tfidf.fit_transform(df['preprocessed'].dropna())\n",
    "y = data.index\n",
    "feature_names = np.array(tfidf.get_feature_names_out())\n",
    "\n",
    "for idx, feature in enumerate(data):\n",
    "    responses = tfidf.transform([features[idx]])\n",
    "    print([get_top_tf_idf_words(response, n) for response in responses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASSIFYING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### choose which media to evaluate\n",
    "# OPTIONS: NachDenkSeiten, Spiegel, BILD, Junge Freiheit\n",
    "first = 'Spiegel'\n",
    "second = 'Junge Freiheit'\n",
    "\n",
    "### set visualization parameters\n",
    "sns.set(style='darkgrid')\n",
    "sns.set_palette('viridis_r')\n",
    "\n",
    "### load data\n",
    "seed = 42 #rng seed\n",
    "n = 300 #number of data points per medium\n",
    "k = 10 #number of cross validation folds\n",
    "df_first = pd.read_csv('data/' + first + '_preprocessed.csv', index_col=0)\n",
    "df_second = pd.read_csv('data/' + second + '_preprocessed.csv', index_col=0)\n",
    "\n",
    "### sample and shuffle data\n",
    "first_sample = df_first.sample(n=n, random_state=seed)\n",
    "first_sample['label'] = first\n",
    "second_sample = df_second.sample(n=n, random_state=seed)\n",
    "second_sample['label'] = second\n",
    "df = pd.concat([first_sample, second_sample])\n",
    "df = shuffle(df, random_state=seed).astype(str)\n",
    "print(df.head())\n",
    "\n",
    "### split data into train and test parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['preprocessed'], df['label'], test_size=0.2, random_state=seed)\n",
    "\n",
    "### vectorize data and fit and transform model\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "### instantiate classifier model and fit to training data\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_bow, y_train)\n",
    "y_pred = clf.predict(X_test_bow)\n",
    "\n",
    "### calculate classification accuracy with k-fold cross validation\n",
    "scores = cross_val_score(clf, X_test_bow, y_test, cv=k)\n",
    "print('cross validation scores:\\n' + str(scores))\n",
    "print('mean cv score: ' + str(scores.mean()))\n",
    "print('confusion matrix:\\n' + str(confusion_matrix(y_test, y_pred)))\n",
    "\n",
    "### data visualization utilizing pca\n",
    "pca = TruncatedSVD(n_components=2)\n",
    "features = pca.fit_transform(X_train_bow)\n",
    "xs = features[:,0]\n",
    "ys = features[:,1]\n",
    "sns.scatterplot(x=xs, y=ys, hue=y_train, alpha=0.5, palette='viridis_r')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "72a455e608f17fa766b2cd98175d031ccf19aaf9f63c199be6fd112f594438bb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
