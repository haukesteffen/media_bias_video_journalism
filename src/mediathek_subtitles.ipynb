{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysrt\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from pandarallel import pandarallel\n",
    "from datetime import datetime\n",
    "\n",
    "clean_pattern = re.compile(r\"<font color=\\\"#[0123456789ABCDEFabcdef]{6}\\\">|</font>\", re.DOTALL)\n",
    "date_pattern = re.compile(r\"Datum:\\s+([0123][0123456789].[01][0123456789].20[012][0123456789])(?=\\n)\", re.DOTALL)\n",
    "description_pattern = re.compile(r\"(?:.m3u8\\n\\n|.mp4\\n\\n)(.*)(?=\\n\\n|\\n)\", re.DOTALL)\n",
    "title_pattern = re.compile(r\"Titel:\\s+(.*)(?=\\n\\nDatum)\", re.DOTALL)\n",
    "show_pattern = re.compile(r\"Thema:\\s+(.*)(?=\\n\\nTitel)\", re.DOTALL)\n",
    "duration_pattern = re.compile(r\"Dauer:\\s+(.*)(?=\\n\\nTitel|\\n\\n\\nWebsite|\\nGröße)\", re.DOTALL)\n",
    "channel_pattern = re.compile(r\"Sender:\\s+([AZ][RD][DF](?=\\n))\", re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_print(verbose=True):\n",
    "    if verbose:\n",
    "        verboseprint = print\n",
    "    else:\n",
    "        verboseprint = lambda *args: None\n",
    "    return verboseprint\n",
    "\n",
    "def load_filter():\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "    filterwords = spacy.lang.de.stop_words.STOP_WORDS\n",
    "    with open(\"../assets/filterwords.txt\", encoding=\"utf-8\", errors=\"ignore\") as d:\n",
    "        filterwords.update(d.read().split())\n",
    "    with open(\"../assets/german_stopwords_full.txt\", encoding=\"utf-8\", errors=\"ignore\") as d:\n",
    "        filterwords.update(d.read().split()[53:])\n",
    "    return list(set(filterwords))\n",
    "\n",
    "def lemmatize(text, nlp, filterwords):\n",
    "    \"\"\"\n",
    "    tokenizes and lemmatizes german input text\n",
    "    :param text: raw input text (german)\n",
    "    :return: list of lemmatized tokens from input text\n",
    "    \"\"\"\n",
    "\n",
    "    with nlp.select_pipes(enable=\"lemmatizer\"):\n",
    "        doc = nlp(text)\n",
    "    lemmas = [token.lemma_.lower() for token in doc]\n",
    "    lemmas = [lemma for lemma in lemmas if lemma.isalpha() and lemma not in filterwords]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "def preprocess(df, to_csv=False, to_pickle=False, verbose=True):\n",
    "    verboseprint = define_print(verbose=verbose)\n",
    "    pandarallel.initialize(progress_bar=True)\n",
    "    filterwords = load_filter()\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    verboseprint(f\"lemmatizing transcript data of {len(df.index)} videos...\")\n",
    "    df[\"preprocessed\"] = df[\"transcript\"].parallel_apply(\n",
    "        lemmatize, args=(nlp, filterwords)\n",
    "    )\n",
    "\n",
    "    if to_csv:\n",
    "        df.to_csv(\"data/preprocessed/\" + df.iloc[0][\"medium\"] + \"_preprocessed.csv\")\n",
    "\n",
    "    if to_pickle:\n",
    "        df.to_pickle(\"data/preprocessed/\" + df.iloc[0][\"medium\"] + \"_preprocessed.pkl\")\n",
    "    return df\n",
    "\n",
    "def convert_string_to_seconds(string):\n",
    "    t = datetime.strptime(string, '%H:%M:%S')\n",
    "    secs = 3600*t.hour + 60*t.minute + t.second\n",
    "    return secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:07<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully created dataframe with 24284 minutes of transcript data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "failcounter = 0\n",
    "mediathek_dict = {\n",
    "    'medium':[],\n",
    "    'id':[],\n",
    "    'title':[],\n",
    "    'description':[],\n",
    "    'duration':[],\n",
    "    'date':[],\n",
    "    'category':[],\n",
    "    'minute':[],\n",
    "    'transcript':[],\n",
    "}\n",
    "\n",
    "folder_list = os.listdir('../assets/mediathek_subtitles/')\n",
    "if '.DS_Store' in folder_list:\n",
    "    folder_list.remove('.DS_Store')\n",
    "\n",
    "for folder in tqdm(folder_list):\n",
    "    for txtfile in glob.glob(\"../assets/mediathek_subtitles/\"+folder+\"/*.txt\"):\n",
    "        try:\n",
    "            with open(txtfile) as f:\n",
    "                txt = f.read()\n",
    "            subs = pysrt.open(txtfile.replace('.txt', '.srt'))\n",
    "        except:\n",
    "            failcounter += 1\n",
    "            continue\n",
    "        \n",
    "        idx = 1\n",
    "        max_minute = max([subs[i].start.minutes for i in range(len(subs))])\n",
    "        \n",
    "        for minute in range(max_minute):\n",
    "            text = \"\"\n",
    "            while subs[idx].start.minutes == minute:\n",
    "                text += re.sub(clean_pattern, \"\", subs[idx].text).replace(\"\\n\",\" \") + \" \"\n",
    "                idx += 1\n",
    "            mediathek_dict['medium'].append(re.findall(channel_pattern, txt)[0])\n",
    "            mediathek_dict['id'].append(re.findall(show_pattern, txt)[0])\n",
    "            mediathek_dict['title'].append(re.findall(title_pattern, txt)[0])\n",
    "            mediathek_dict['description'].append(re.findall(description_pattern, txt)[0].replace(\"\\n\", \" \"))\n",
    "            mediathek_dict['duration'].append(re.findall(duration_pattern, txt)[0])\n",
    "            mediathek_dict['date'].append(re.findall(date_pattern, txt)[0])\n",
    "            mediathek_dict['category'].append(\"News & Politics\")\n",
    "            mediathek_dict['minute'].append(minute)\n",
    "            mediathek_dict['transcript'].append(text)\n",
    "\n",
    "df = pd.DataFrame(mediathek_dict)\n",
    "print(f\"successfully created dataframe with {len(df.index)} minutes of transcript data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "lemmatizing transcript data of 24284 videos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6518af93e76435992f995f9aee6b89c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=3036), Label(value='0 / 3036'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = preprocess(df)\n",
    "df['duration'] = df['duration'].apply(convert_string_to_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['id'] == 'MONITOR studioM', 'id'] = 'Monitor'\n",
    "df.loc[df['id'] == 'Menschen bei Maischberger', 'id'] = 'Maischberger'\n",
    "df.loc[df['id'] == 'maischberger. die woche', 'id'] = 'Maischberger'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('../data/mediathek_data.pkl')\n",
    "df_mediathek = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube = pd.read_pickle('../data/topic.pkl')\n",
    "df_combined = pd.concat([df_mediathek, df_youtube], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_pickle('../data/topics_combined_old.pkl')\n",
    "df_combined.index = test_df.index\n",
    "df_combined['topic'] = test_df['topic']\n",
    "df_combined.to_pickle('../data/topics_combined.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mediathek['topic'] = df_combined['topic'].iloc[0:24283]\n",
    "df_mediathek.to_pickle('../data/mediathek_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Monitor', 'frontal', 'Maischberger', 'Tagesthemen',\n",
       "       'maybrit illner', 'Markus Lanz', 'Frontal 21', 'Hart aber fair',\n",
       "       'Anne Will'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mediathek['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['frontal', 'maybrit illner', 'Markus Lanz', 'Frontal 21'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mediathek.loc[df['medium'] == 'ZDF', 'id'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ae7ae13804f56b6812076ff88d4c743516b7c995d69dd5984882be015e04ad2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
