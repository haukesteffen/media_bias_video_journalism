{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "linebreak_pattern_1 = re.compile(r\"-\\n\\d+\\n\", re.DOTALL)\n",
    "linebreak_pattern_2 = re.compile(r\"-\\n\", re.DOTALL)\n",
    "space_pattern = re.compile(r\"\\n|\\x0c|\\x07|\\x08|\\xad|\\xa0|\\u200a|\\t|\\.|\\,|\\/\", re.DOTALL)\n",
    "word_pattern = re.compile(r\"[a-zA-Z üäößÜÖÄ]\", re.DOTALL)\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "\n",
    "political_vocabulary = [\n",
    "    'freiheit',\n",
    "    'gleich',\n",
    "    'solidar',\n",
    "    'sozial',\n",
    "    'privatisier',\n",
    "    'leistung',\n",
    "    'heimat',\n",
    "    'mitbestimm',\n",
    "    'nachhaltig',\n",
    "    'umwelt',\n",
    "    'armut',\n",
    "    'diskriminier',\n",
    "    'demokratisier',\n",
    "    'digital',\n",
    "    'klima',\n",
    "    #-----\n",
    "    'familie',\n",
    "    'zuwander',\n",
    "    'ezb',\n",
    "    'national',\n",
    "    'islam',\n",
    "    #-----\n",
    "    'chance',\n",
    "    'bevölker',\n",
    "    'beschleunig',\n",
    "    'modernisier',\n",
    "    'wachstum',\n",
    "    #-----\n",
    "    'menschenrecht',\n",
    "    'bürokrat',\n",
    "    'innovation',\n",
    "    'selbstständig',\n",
    "    'liberal',\n",
    "    #-----\n",
    "    'respekt',\n",
    "    'jugendlich',\n",
    "    'krise',\n",
    "    'engagement',\n",
    "    'gewerkschaft',\n",
    "    #-----\n",
    "    'klimakrise',\n",
    "    'sozialökolog',\n",
    "    'geflüchtet',\n",
    "    'klimaneutral',\n",
    "    'weiterbild',\n",
    "    #-----\n",
    "    'beschäftigt',\n",
    "    'konzern',\n",
    "    'gerecht',\n",
    "    'wohn',\n",
    "    'löhne','lohn',\n",
    "]\n",
    "\n",
    "parties = {\n",
    "    'linke':[],\n",
    "    'grüne':[],\n",
    "    'spd':[],\n",
    "    'fdp':[],\n",
    "    'cdu_csu':[],\n",
    "    'afd':[],   \n",
    "}\n",
    "\n",
    "media = [\n",
    "    'junge Welt',\n",
    "    \"NachDenkSeiten\",\n",
    "    'taz',\n",
    "    'Süddeutsche Zeitung',\n",
    "    'stern TV',\n",
    "    \"DER SPIEGEL\",\n",
    "    'ZEIT ONLINE',\n",
    "    'Der Tagesspiegel',\n",
    "    'ARD',\n",
    "    #'Tagesschau',\n",
    "    'ZDF',\n",
    "    \"ZDFheute Nachrichten\",\n",
    "    'Bayerischer Rundfunk',\n",
    "    'ntv Nachrichten',\n",
    "    'RTL',\n",
    "    'FOCUS Online',\n",
    "    'faz',\n",
    "    'WELT',\n",
    "    \"BILD\",\n",
    "    'NZZ Neue Zürcher Zeitung',\n",
    "    \"Junge Freiheit\",\n",
    "    'COMPACTTV'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_party_phrase_counts(party):\n",
    "    with open(f\"../assets/manifests/{party}_manifest.txt\", encoding=\"utf-8\", errors=\"ignore\") as d:\n",
    "        manifest = d.read()\n",
    "    manifest = re.sub(linebreak_pattern_1, \"\", manifest)\n",
    "    manifest = re.sub(linebreak_pattern_2, \"\", manifest)\n",
    "    manifest = re.sub(space_pattern, \" \", manifest)\n",
    "    manifest = \"\".join(re.findall(word_pattern, manifest))\n",
    "    manifest = re.sub(whitespace_pattern, \" \", manifest)\n",
    "    with nlp.select_pipes(enable=\"lemmatizer\"):\n",
    "        doc = nlp(manifest)\n",
    "    lemmas = [token.lemma_.lower() for token in doc]\n",
    "    manifest_vocabulary = [lemma for lemma in lemmas if lemma.isalpha()]\n",
    "    phrase_counts = {phrase: 0 for phrase in political_vocabulary}\n",
    "    for political_phrase in political_vocabulary:\n",
    "        for manifest_phrase in manifest_vocabulary:\n",
    "            if manifest_phrase.__contains__(political_phrase):\n",
    "                phrase_counts[political_phrase] += 1\n",
    "    phrase_counts['total'] = len(manifest_vocabulary)\n",
    "    return pd.Series(phrase_counts)\n",
    "\n",
    "def extract_medium_phrase_counts(df, medium):\n",
    "    phrase_counts = {phrase: 0 for phrase in political_vocabulary}\n",
    "    for doc in df[df['medium'] == medium]['transcript']:\n",
    "        with nlp.select_pipes(enable=\"lemmatizer\"):\n",
    "            preprocessed = nlp(doc)\n",
    "        lemmas = [token.lemma_.lower() for token in preprocessed]\n",
    "        medium_vocabulary = [lemma for lemma in lemmas if lemma.isalpha()]\n",
    "        for political_phrase in political_vocabulary:\n",
    "            for medium_phrase in medium_vocabulary:\n",
    "                if medium_phrase.__contains__(political_phrase):\n",
    "                    phrase_counts[political_phrase] += 1\n",
    "    return pd.Series(phrase_counts)\n",
    "\n",
    "def standardize_df(input_df):\n",
    "    df = input_df.copy()\n",
    "    for party in parties.keys():\n",
    "        df[party] -= df[party].mean()\n",
    "    return df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for party in parties.keys():\n",
    "    with open(f\"../assets/manifests/{party}_manifest.txt\", encoding=\"utf-8\", errors=\"ignore\") as d:\n",
    "        manifest = d.read()\n",
    "    manifest = re.sub(linebreak_pattern_1, \"\", manifest)\n",
    "    manifest = re.sub(linebreak_pattern_2, \"\", manifest)\n",
    "    manifest = re.sub(space_pattern, \" \", manifest)\n",
    "    manifest = \"\".join(re.findall(word_pattern, manifest))\n",
    "    manifest = re.sub(whitespace_pattern, \" \", manifest)\n",
    "    with nlp.select_pipes(enable=\"lemmatizer\"):\n",
    "        doc = nlp(manifest)\n",
    "    lemmas = [token.lemma_.lower() for token in doc]\n",
    "    parties[party] = [lemma for lemma in lemmas if lemma.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer(min_df=2, max_df=5)\n",
    "test = vec.fit_transform([\" \".join(i) for _, i in parties.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "features = vec.get_feature_names_out()\n",
    "for i in range(6):\n",
    "    sorted = np.argsort(test[i,:].toarray()).flatten()[::-1]\n",
    "    top_n = features[sorted][:n]\n",
    "    print(f\"party: {list(parties.keys())[i]}\\ndiscriminative phrases:\\n{top_n}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_dict = {\n",
    "    party:extract_party_phrase_counts(party) for party in tqdm(parties.keys())\n",
    "}\n",
    "party_df = pd.DataFrame(phrase_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/topics_combined.pkl')\n",
    "phrase_dict = {\n",
    "    medium:extract_medium_phrase_counts(df, medium) for medium in tqdm(media)\n",
    "}\n",
    "media_df = pd.DataFrame(phrase_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_similarity_dict = {}\n",
    "for party in parties:\n",
    "    medium_similarity_dict = {}\n",
    "    for medium in media:\n",
    "        medium_similarity_dict[medium] = cosine_similarity([party_df[party].iloc[:-1].to_list(), media_df[medium].to_list()])[0, 1]\n",
    "    medium_series = pd.Series(medium_similarity_dict) \n",
    "    party_similarity_dict[party] = medium_series\n",
    "similarity_df = pd.DataFrame(party_similarity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_plot = similarity_df.drop('Tagesschau')\n",
    "\n",
    "f, ax = plt.subplots(figsize=(9, 8))\n",
    "sns.heatmap(df_to_plot, annot=True, linewidths=.5, ax=ax, fmt=\".2\", center=np.nanmean(df_to_plot))\n",
    "ax.set(xlabel='party', ylabel='medium', title='cosine similarity between party manifests and media tf-idf vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_plot = standardize_df(similarity_df.drop('Tagesschau'))\n",
    "\n",
    "f, ax = plt.subplots(figsize=(9, 8))\n",
    "sns.heatmap(df_to_plot, annot=True, linewidths=.5, ax=ax, fmt=\".2\", center=np.nanmean(df_to_plot))\n",
    "ax.set(xlabel='party', ylabel='medium', title='cosine similarity between party manifests and media tf-idf vocabulary, standardized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = similarity_df.drop('Tagesschau')\n",
    "print(f\"cosine similarity means:\\n{stats_df.mean()}\\n\\ncosine similarity standard deviation:\\n{stats_df.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df.drop('Tagesschau').loc[media].to_pickle('../data/vocabulary/tfidf.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ae7ae13804f56b6812076ff88d4c743516b7c995d69dd5984882be015e04ad2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
