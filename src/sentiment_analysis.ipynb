{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=False)\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "media = [\n",
    "    'junge Welt',\n",
    "    \"NachDenkSeiten\",\n",
    "    'taz',\n",
    "    'Süddeutsche Zeitung',\n",
    "    'stern TV',\n",
    "    \"DER SPIEGEL\",\n",
    "    'ZEIT ONLINE',\n",
    "    'Der Tagesspiegel',\n",
    "    'ARD',\n",
    "    'tagesschau',\n",
    "    'ZDF',\n",
    "    \"ZDFheute Nachrichten\",\n",
    "    'Bayerischer Rundfunk',\n",
    "    'ntv Nachrichten',\n",
    "    'RTL',\n",
    "    'FOCUS Online',\n",
    "    'faz',\n",
    "    'WELT',\n",
    "    \"BILD\",\n",
    "    'NZZ Neue Zürcher Zeitung',\n",
    "    \"Junge Freiheit\",\n",
    "    'COMPACTTV'\n",
    "]\n",
    "\n",
    "search_terms = {\n",
    "    'cdu':['cdu', 'union'],\n",
    "    'csu':['csu', 'union'],\n",
    "    'fdp':['fdp', 'freien demokraten'],\n",
    "    'grüne':['grünen'],\n",
    "    'linke':['linke', 'linkspartei'],\n",
    "    'afd':['afd', 'afg'],\n",
    "    'spd':['spd', 'sozialdemokraten'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_party_mentions(input_string, party_strings, n_words=10):\n",
    "    input_string = input_string.lower()\n",
    "    party_boolean = [False for i in input_string.split()]\n",
    "    for p in party_strings:\n",
    "        temp = [string.__contains__(p) for string in input_string.split()]\n",
    "        party_boolean = [party_boolean|temp for (party_boolean,temp) in zip(party_boolean, temp)]\n",
    "    party_index = np.where(party_boolean)[0]\n",
    "    output_strings = ['' for _ in range(len(party_index))]\n",
    "    for i in range(len(party_index)):\n",
    "        lower_bound = party_index[i]-n_words\n",
    "        upper_bound = party_index[i]+n_words\n",
    "        if lower_bound < 0:\n",
    "            lower_bound = 0\n",
    "        if upper_bound > len(input_string.split()):\n",
    "            upper_bound = len(input_string.split())\n",
    "        output_strings[i] = \" \".join(input_string.split()[lower_bound:upper_bound])\n",
    "    return output_strings\n",
    "\n",
    "def extract_mention_df(party):\n",
    "    subset = 'contains_' + party\n",
    "    res_series = df['transcript'].loc[df[subset]].parallel_apply(lambda transcript: extract_party_mentions(input_string=transcript, party_strings=search_terms[party], n_words=10))\n",
    "    temp = {'medium': df['medium'].loc[df[subset]], 'transcript':res_series}\n",
    "    res_df = pd.DataFrame(temp).explode(column='transcript')\n",
    "    res_df.reset_index(inplace=True)\n",
    "    res_df.dropna(inplace=True)\n",
    "    res_df.drop(res_df.index[res_df['transcript'] == ''], inplace=True)\n",
    "    return res_df\n",
    "\n",
    "def extract_sentiment_df(input_df):\n",
    "    input_df['sentiment'] = input_df['transcript'].progress_apply(classifier)\n",
    "    input_df['positive'] = [True if sent[0]['label']=='positive' else False for sent in input_df['sentiment']]\n",
    "    input_df['neutral'] = [True if sent[0]['label']=='neutral' else False for sent in input_df['sentiment']]\n",
    "    input_df['negative'] = [True if sent[0]['label']=='negative' else False for sent in input_df['sentiment']]\n",
    "    input_df['score'] = [sent[0]['score'] for sent in input_df['sentiment']]\n",
    "    return input_df\n",
    "\n",
    "def extract_avg_sentiment_df(input_df):\n",
    "    avg_pos = input_df[input_df['positive']].groupby(['medium'])['score'].median()\n",
    "    avg_neu = input_df[input_df['neutral']].groupby(['medium'])['score'].median()\n",
    "    avg_neg = input_df[input_df['negative']].groupby(['medium'])['score'].median()\n",
    "    output_df = pd.DataFrame(data={'avg_pos':avg_pos, 'avg_neu':avg_neu, 'avg_neg':avg_neg})\n",
    "    return output_df\n",
    "\n",
    "def extract_sentiment_counts_df(input_df):\n",
    "    count_pos = input_df[input_df['positive']].groupby(['medium'])['positive'].sum()\n",
    "    count_neu = input_df[input_df['neutral']].groupby(['medium'])['neutral'].sum()\n",
    "    count_neg = input_df[input_df['negative']].groupby(['medium'])['negative'].sum()\n",
    "    output_df = pd.DataFrame(data={'count_pos':count_pos, 'count_neu':count_neu, 'count_neg':count_neg})\n",
    "    return output_df\n",
    "\n",
    "def get_avg_party_sentiment(party):\n",
    "    df = extract_mention_df(party)\n",
    "    df = extract_sentiment_df(df)\n",
    "    df = extract_avg_sentiment_df(df)\n",
    "    return df['avg_pos'], df['avg_neu'], df['avg_neg']\n",
    "\n",
    "def get_party_sentiment_counts(party):\n",
    "    df = extract_mention_df(party)\n",
    "    df = extract_sentiment_df(df)\n",
    "    df = extract_sentiment_counts_df(df)\n",
    "    return df['count_pos'], df['count_neu'], df['count_neg']\n",
    "\n",
    "def standardize_df(input_df):\n",
    "    df = input_df.copy()\n",
    "    for party in search_terms.keys():\n",
    "        df[party] -= df[party].mean()\n",
    "    return df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_computation = True\n",
    "\n",
    "df = pd.read_pickle('../data/topics_combined.pkl')\n",
    "for party in search_terms.keys():\n",
    "    df['contains_'+party] = [False for _ in range(len(df.index))]\n",
    "    for term in search_terms[party]:\n",
    "        df['contains_'+party] = df['contains_'+party] | df['preprocessed'].str.contains(term)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mdraw/german-news-sentiment-bert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"mdraw/german-news-sentiment-bert\")\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'Olaf Scholz hielt eine Rede im Bundestag'\n",
    "print(f\"classification: {classifier(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 2\n",
    "party = 'spd'\n",
    "n_words = 20\n",
    "\n",
    "subset = 'contains_' + party\n",
    "teststring = df.loc[df[subset]].iloc[ind]['preprocessed']\n",
    "extracted_strings = extract_party_mentions(input_string=teststring, party_strings=search_terms[party], n_words=n_words)\n",
    "print(f'Strings: {extracted_strings}')\n",
    "print(f'Classification: {classifier(extracted_strings)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skip_computation:\n",
    "    pos_counts_df = pd.read_pickle('../data/sentiment/pos_counts_df.pkl')\n",
    "    neu_counts_df = pd.read_pickle('../data/sentiment/neu_counts_df.pkl')\n",
    "    neg_counts_df = pd.read_pickle('../data/sentiment/neg_counts_df.pkl')\n",
    "else:\n",
    "    pos_counts_dict = {}\n",
    "    neu_counts_dict = {}\n",
    "    neg_counts_dict = {}\n",
    "    for party in search_terms.keys():\n",
    "        pos_counts_dict[party], neu_counts_dict[party], neg_counts_dict[party] = get_party_sentiment_counts(party)\n",
    "\n",
    "    pos_counts_df = pd.DataFrame(pos_counts_dict)\n",
    "    pos_counts_df = pos_counts_df.loc[media]\n",
    "    pos_counts_df = pos_counts_df[['linke', 'grüne', 'spd', 'cdu', 'csu', 'fdp', 'afd']]\n",
    "    neu_counts_df = pd.DataFrame(neu_counts_dict)\n",
    "    neu_counts_df = neu_counts_df.loc[media]\n",
    "    neu_counts_df = neu_counts_df[['linke', 'grüne', 'spd', 'cdu', 'csu', 'fdp', 'afd']]\n",
    "    neg_counts_df = pd.DataFrame(neg_counts_dict)\n",
    "    neg_counts_df = neg_counts_df.loc[media]\n",
    "    neg_counts_df = neg_counts_df[['linke', 'grüne', 'spd', 'cdu', 'csu', 'fdp', 'afd']]\n",
    "\n",
    "    pos_counts_df.to_pickle('../data/sentiment/pos_counts_df.pkl')\n",
    "    neu_counts_df.to_pickle('../data/sentiment/neu_counts_df.pkl')\n",
    "    neg_counts_df.to_pickle('../data/sentiment/neg_counts_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pos_counts_df.copy()\n",
    "\n",
    "for party in search_terms.keys():\n",
    "    temp_df = extract_mention_df(party)['medium'].value_counts().to_dict()\n",
    "    for medium in media:\n",
    "        pos.loc[medium, party] /= temp_df[medium]\n",
    "\n",
    "f, ax = plt.subplots(figsize=(9, 8))\n",
    "sns.heatmap(pos, annot=True, linewidths=.5, ax=ax, fmt=\".1%\", center=np.nanmean(pos))\n",
    "ax.set(xlabel='party', ylabel='medium', title='positive sentiment proportions by medium and party')\n",
    "\n",
    "f, ax = plt.subplots(figsize=(9, 8))\n",
    "sns.heatmap(standardize_df(pos), annot=True, linewidths=.5, ax=ax, fmt=\".1%\", center=np.nanmean(standardize_df(pos)))\n",
    "ax.set(xlabel='party', ylabel='medium', title='positive sentiment proportions by medium and party, standardized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"positive sentiment means:\\n{pos.mean()}\\n\\npositive sentiment standard deviation:\\n{pos.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neu = neu_counts_df.copy()\n",
    "\n",
    "for party in search_terms.keys():\n",
    "    temp_df = extract_mention_df(party)['medium'].value_counts().to_dict()\n",
    "    for medium in media:\n",
    "        neu.loc[medium, party] /= temp_df[medium]\n",
    "\n",
    "f, ax = plt.subplots(figsize=(9, 8))\n",
    "sns.heatmap(neu, annot=True, linewidths=.5, ax=ax, fmt=\".1%\", center=np.nanmean(neu))\n",
    "ax.set(xlabel='party', ylabel='medium', title='neutral sentiment proportions by medium and party')\n",
    "\n",
    "f, ax = plt.subplots(figsize=(9, 8))\n",
    "sns.heatmap(standardize_df(neu), annot=True, linewidths=.5, ax=ax, fmt=\".1%\", center=np.nanmean(standardize_df(neu)))\n",
    "ax.set(xlabel='party', ylabel='medium', title='neutral sentiment proportions by medium and party, standardized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"neutral sentiment means:\\n{neu.mean()}\\n\\nneutral sentiment standard deviation:\\n{neu.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = neg_counts_df.copy()\n",
    "\n",
    "for party in search_terms.keys():\n",
    "    temp_df = extract_mention_df(party)['medium'].value_counts().to_dict()\n",
    "    for medium in media:\n",
    "        neg.loc[medium, party] /= temp_df[medium]\n",
    "\n",
    "f, ax = plt.subplots(figsize=(9, 8))\n",
    "sns.heatmap(neg, annot=True, linewidths=.5, ax=ax, fmt=\".1%\", center=np.nanmean(neg))\n",
    "ax.set(xlabel='party', ylabel='medium', title='negative sentiment proportions by medium and party')\n",
    "\n",
    "f, ax = plt.subplots(figsize=(9, 8))\n",
    "sns.heatmap(standardize_df(neg), annot=True, linewidths=.5, ax=ax, fmt=\".1%\", center=np.nanmean(standardize_df(neg)))\n",
    "ax.set(xlabel='party', ylabel='medium', title='negative sentiment proportions by medium and party, standardized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"negative sentiment means:\\n{neg.mean()}\\n\\nnegative sentiment standard deviation:\\n{neg.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent, df in dict(zip(['pos', 'neu', 'neg'],[pos, neu, neg])).items():\n",
    "    export_df = df.copy(deep=True)\n",
    "    export_df = df.loc[media].drop('tagesschau')[['linke', 'grüne', 'spd', 'fdp', 'cdu', 'csu', 'afd']]\n",
    "    export_df.to_pickle(f'../data/sentiment/{sent}.pkl')\n",
    "\n",
    "    export_df_mwf = df.copy(deep=True)\n",
    "    export_df_mwf = standardize_df(df).loc[media].drop('tagesschau')[['linke', 'grüne', 'spd', 'fdp', 'cdu', 'csu', 'afd']]\n",
    "    export_df_mwf.to_pickle(f'../data/sentiment/{sent}_mittelwertfrei.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ae7ae13804f56b6812076ff88d4c743516b7c995d69dd5984882be015e04ad2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
